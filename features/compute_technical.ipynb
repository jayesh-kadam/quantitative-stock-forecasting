{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-17T11:35:57.790251Z",
     "start_time": "2025-06-17T11:35:55.312075Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "compute_technical.py\n",
    "\n",
    "Computes technical indicators from price_data and writes the results to a new PostgreSQL table.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "\n",
    "# Setup DB connection\n",
    "engine = create_engine(DATABASE_URL)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Load price data\n",
    "query = \"\"\"\n",
    "SELECT ticker, date, close, volume\n",
    "FROM price_data\n",
    "ORDER BY ticker, date ASC;\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine, parse_dates=[\"date\"])\n",
    "\n",
    "# Compute technical indicators per ticker\n",
    "def compute_indicators(group):\n",
    "    group = group.sort_values(\"date\").copy()\n",
    "    group[\"return_1d\"] = group[\"close\"].pct_change()\n",
    "    group[\"sma_5\"] = group[\"close\"].rolling(window=5).mean()\n",
    "    group[\"sma_20\"] = group[\"close\"].rolling(window=20).mean()\n",
    "    group[\"ema_10\"] = group[\"close\"].ewm(span=10, adjust=False).mean()\n",
    "    group[\"rsi_14\"] = compute_rsi(group[\"close\"], 14)\n",
    "    group[\"macd\"] = group[\"close\"].ewm(span=12, adjust=False).mean() - group[\"close\"].ewm(span=26, adjust=False).mean()\n",
    "    return group\n",
    "\n",
    "# RSI function\n",
    "def compute_rsi(series, period=14):\n",
    "    delta = series.diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(window=period).mean()\n",
    "    loss = -delta.where(delta < 0, 0).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "# Clean and apply without warning or dropping 'ticker'\n",
    "features_df = (\n",
    "    df.groupby(\"ticker\", group_keys=False)\n",
    "    .apply(compute_indicators)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Drop duplicates before inserting\n",
    "unique_keys = features_df[['ticker', 'date']].drop_duplicates()\n",
    "\n",
    "for i in range(len(unique_keys)):\n",
    "    ticker = unique_keys.iloc[i]['ticker']\n",
    "    date_val = pd.to_datetime(unique_keys.iloc[i]['date']).date()\n",
    "\n",
    "    session.execute(\n",
    "        text(\"DELETE FROM technical_features WHERE ticker = :ticker AND date = :date\"),\n",
    "        {\"ticker\": ticker, \"date\": date_val}\n",
    "    )\n",
    "session.commit()\n",
    "\n",
    "# Append new data\n",
    "features_df.to_sql('technical_features', engine, if_exists='append', index=False)\n",
    "print(\"✅ Technical features deduplicated and saved to 'technical_features' table in PostgreSQL\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3y/jcf5sjb1319blyc1b_cfn1xm0000gn/T/ipykernel_66624/3128870013.py:52: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_indicators)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Technical features deduplicated and saved to 'technical_features' table in PostgreSQL\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T11:35:58.234744Z",
     "start_time": "2025-06-17T11:35:57.799256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "merge_features.py\n",
    "\n",
    "Uses merge_asof for fundamental and macroeconomic time-aware joins.\n",
    "Merges technical, sentiment, macro, and fundamental features into a single table `merged_features`.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Load env vars and setup engine\n",
    "load_dotenv()\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "def safe_load_sql(table, date_col=\"date\"):\n",
    "    try:\n",
    "        return pd.read_sql(f\"SELECT * FROM {table}\", engine, parse_dates=[date_col])\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipping {table}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load tables\n",
    "technical_df = safe_load_sql(\"technical_features\")\n",
    "sentiment_df = safe_load_sql(\"sentiment_data\")\n",
    "macro_df = safe_load_sql(\"macro_data\")\n",
    "fundamentals_df = safe_load_sql(\"fundamental_data\")\n",
    "\n",
    "# Aggregate sentiment\n",
    "if not sentiment_df.empty:\n",
    "    sentiment_agg = sentiment_df.groupby([\"ticker\", \"date\"]).agg(\n",
    "        sentiment_avg=(\"sentiment_score\", \"mean\"),\n",
    "        sentiment_std=(\"sentiment_score\", \"std\"),\n",
    "        sentiment_count=(\"sentiment_score\", \"count\")\n",
    "    ).reset_index()\n",
    "else:\n",
    "    sentiment_agg = pd.DataFrame()\n",
    "\n",
    "# Merge base technical + sentiment\n",
    "merged = pd.merge(technical_df, sentiment_agg, on=[\"ticker\", \"date\"], how=\"left\")\n",
    "merged = merged.sort_values([\"ticker\", \"date\"])\n",
    "\n",
    "# Merge fundamentals using merge_asof\n",
    "if not fundamentals_df.empty and \"metric\" in fundamentals_df.columns:\n",
    "    fundamentals_df = fundamentals_df.pivot(index=[\"symbol\", \"date\"], columns=\"metric\", values=\"value\").reset_index()\n",
    "    fundamentals_df.rename(columns={\"symbol\": \"ticker\"}, inplace=True)\n",
    "    fundamentals_df = fundamentals_df.sort_values([\"ticker\", \"date\"])\n",
    "    merged = pd.merge_asof(\n",
    "        merged, fundamentals_df,\n",
    "        by=\"ticker\", on=\"date\",\n",
    "        direction=\"backward\"\n",
    "    )\n",
    "\n",
    "# Merge macro data using merge_asof\n",
    "if not macro_df.empty and \"indicator_name\" in macro_df.columns:\n",
    "    macro_df = macro_df.pivot(index=\"date\", columns=\"indicator_name\", values=\"value\").reset_index()\n",
    "    macro_df = macro_df.sort_values(\"date\")\n",
    "    merged = pd.merge_asof(\n",
    "        merged, macro_df,\n",
    "        on=\"date\",\n",
    "        direction=\"backward\"\n",
    "    )\n",
    "\n",
    "# Add merge timestamp\n",
    "merged[\"merged_at\"] = datetime.now(timezone.utc)\n",
    "\n",
    "# Save to DB\n",
    "merged.to_sql(\"merged_features\", engine, if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Saved {len(merged)} rows to 'merged_features' table.\")\n"
   ],
   "id": "7161d1f16c36a0f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 6641 rows to 'merged_features' table.\n"
     ]
    }
   ],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
